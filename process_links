#! /usr/bin/perl -w
use strict;
use warnings;
use utf8;
use Encode;
use File::Basename qw/dirname/;
binmode STDOUT, ":utf8";
chdir(dirname($0));
use DBI;
use Digest::MD5;
use HTML::LinkExtractor;
use URI;
use Data::Dumper;
use Getopt::Std;
use util::Io;
use util::Errors;
use util::String;
use Spamfilter;
use Converter;
use Extractor;
my %cfg = do 'config.pl';

# A few more config settings:
$cfg{'TEMPDIR'}       = $cfg{'PATH'}.'temp/';
$cfg{'RPDF'}          = $cfg{'PATH'}.'rpdf/rpdf';
$cfg{'SPAMCORPUS'}    = $cfg{'PATH'}.'spamcorpus/';
$cfg{'RTF2PDF'}       = $cfg{'PATH'}.'util/rtf2pdf.sh';
# Spam score threshold where we don't store the document:
$cfg{'CERT_SPAM'}     = 0.8;
$cfg{'REPROCESS_ALL'} = 0;

my %opts;
getopts("v:p:nh", \%opts);
if ($opts{h}) {
    print <<EOF;

Fetches documents and tries to guess author, title, abstract, etc.,
and whether they are a suitable paper at all. Run as a cronjob without
arguments, or with arguments for testing and debugging:

Usage: $0 [-hn] [-p url or location id] [-v verbosity]

-p        : url or id that will be processed
-v        : verbosity level (0-10)
-n        : dry run, do not write result to DB
-h        : this message

EOF
    exit;
}

my $verbosity = $opts{v} || 0;
util::Errors::verbosity($verbosity);
util::Io::verbosity($verbosity);

# don't run multiple processes in parallel:
my $lockfile = $cfg{'PATH'}.'.processlock';
if ( -e "$lockfile" ) {
    if ( -M "$lockfile" < 0.005) {
        # modified in the last ~10 mins
        print "process already running (or last call crashed: remove .processlock)\n"
            if $verbosity;
        exit;
    }
    print "killing previous run!\n";
    system("rm -f '$lockfile'");
    # we are killing ourserlves here!
    system("ps -ef | grep 'process_links' | grep -v grep"
           ." | awk '{print \$2}' | xargs kill -9");
}
system("touch '$lockfile'");

# find some documents to check:
my $dbh = DBI->connect('DBI:mysql:'.$cfg{'MYSQL_DB'}, $cfg{'MYSQL_USER'},
    $cfg{'MYSQL_PASS'}, { RaiseError => 1 }) 
    or die "Couldn't connect to database: " . DBI->errstr;
$dbh->{'mysql_auto_reconnect'} = 1;
if ($opts{p}) {
    # from command-line
    my ($url, $id);
    if ($opts{p} =~ /^\d+$/) {
        $id = $opts{p};
 	($url) = $dbh->selectrow_array("SELECT url FROM locations WHERE location_id = $opts{p}");
	die "ID $opts{p} not in database" unless ($url);
    }
    else {
	$url = $opts{p};
        ($id) = $dbh->selectrow_array("SELECT location_id FROM locations WHERE url = '$opts{p}'");
    }
    my @locations = ({ location_id => $id, url => $url });
    process_locations(@locations);
}
else {
    # from database.
    # Do we have unprocessed locations?
    my $query = "SELECT * FROM locations "
        ."WHERE status = 0 ORDER BY location_id LIMIT ".$cfg{'NUM_URLS'};
    my @locations = @{$dbh->selectall_arrayref($query, { Slice => {} })};
    if (!@locations) {
        print "No new locations to check. Verifying some old stuff.\n" if $verbosity;
	# No. Toss a coin to decide whether to (a) verify old papers
	# and re-check locations with HTTP errors, or (b) give old spam
	# and parser errors a new chance. Mostly we want to do (a).
	# But first, check if we reprocess everything.
	if ($cfg{'REPROCESS_ALL'}) {
	    $query = "SELECT *,UNIX_TIMESTAMP(last_checked) AS last_checked FROM locations "
                ."ORDER BY last_checked LIMIT ".$cfg{'NUM_URLS'};
	}
	elsif (rand(10) <= 9) {
	    # (a) re-process old papers and HTTP errors:
	    $query = "SELECT *,UNIX_TIMESTAMP(last_checked) AS last_checked FROM locations "
                ."WHERE (status NOT BETWEEN 2 AND 99) AND NOT spamminess > 0.5 "
                ."ORDER BY last_checked LIMIT ".$cfg{'NUM_URLS'};
	}
	else {
	    # (b) give old spam and parser errors a second chance:
	    $query = "SELECT *,UNIX_TIMESTAMP(last_checked) AS last_checked FROM locations "
                ."WHERE (status = 1 AND spamminess > 0.5) OR status BETWEEN 2 AND 99 "
                ."ORDER BY last_checked LIMIT ".$cfg{'NUM_URLS'};
	}
	@locations = @{$dbh->selectall_arrayref($query, { Slice => {} })};
    }
    process_locations(@locations);
}

print "done.\n" if $verbosity;

system('rm -f '.$cfg{'TEMPDIR'}.'*') unless $verbosity;
system("rm -f '$lockfile'");

$dbh->disconnect() if ($dbh);

sub process_locations {
    my @locations = @_;

    my $really = $opts{n} ? "AND 1 = 0" : "";
    my $db_verify = $dbh->prepare(
        "UPDATE locations SET last_checked = NOW() WHERE location_id = ? $really");
    my $db_err = $dbh->prepare(
        "UPDATE locations SET status = ?, last_checked = NOW() "
        ."WHERE location_id = ? $really");
    my $db_saveloc = $dbh->prepare(
        "UPDATE locations SET status = 1, filetype = ?, filesize = ?, "
        ."spamminess = ?, document_id = ?, last_checked = NOW() "
        ."WHERE location_id = ? $really");
    my $db_savedoc = $dbh->prepare(
        "UPDATE documents SET authors = ?, title = ?, abstract = ?, "
        ."length = ?, meta_confidence = ? WHERE document_id = ? $really");
    my $db_adddoc = $dbh->prepare(
        "INSERT IGNORE INTO documents "
        ."(found_date, authors, title, abstract, length, meta_confidence) "
        ."VALUES (NOW(),?,?,?,?,?)");

    foreach my $loc (@locations) {
        my $loc_id = $loc->{location_id};
	print "checking location $loc_id: $loc->{url}\n" if $verbosity;

        if ($loc->{url} =~ /^https:/) {
	    error("HTTPS is currently not supported");
	    $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
	    next;
        }

	# retrieve document:
	my $mtime = (defined $loc->{last_checked} && !$cfg{'REPROCESS_ALL'}) ? 
            $loc->{last_checked} : 0;
	my $res = fetch_url($loc->{url}, $mtime);
	if ($res && $res->code == 304 && defined $loc->{last_checked}) {
	    print "not modified.\n" if $verbosity;
	    $db_verify->execute($loc_id) or print DBI->errstr;
	    next;
	}
	if (!$res || !$res->is_success) {
	    my $status = $res ? $res->code : 900;
	    print "status $status.\n" if $verbosity;
            $db_err->execute($status, $loc_id) or print DBI->errstr;
	    next;
	}
	if (!$res->content || !$res->{filesize}) {
	    error("document is empty");
	    $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
	    next;
	}
        print "file retrieved, $res->{filesize} bytes\n" if $verbosity;

	# We want to make sure that we only update an old link if it
	# has really (substantially) changed. HTTP headers are not to
	# be trusted on this. So we also check for changes in
	# filesize:
        my $old_filesize = defined $loc->{filesize} ? $loc->{filesize} : 0;
	if ($old_filesize && abs($old_filesize-$res->{filesize})/$res->{filesize} < 0.2 
            && !$cfg{'REPROCESS_ALL'}) {
	    print "no substantial change in filesize.\n" if $verbosity;
	    $db_verify->execute($loc_id) or print DBI->errstr;
	    next;
	}

	$loc->{filesize} = $res->{filesize};
	$loc->{filetype} = $res->{filetype};
	$loc->{content} = $res->{content};

	# check if filetype is supported:
        unless (grep {$loc->{filetype} eq $_} @{$cfg{'FILETYPES'}}) {
	    error("unsupported filetype ".$loc->{filetype}."\n"); 
	    $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
	    next;
	}

	# save local copy:
	my $file = $cfg{'TEMPDIR'}.Digest::MD5::md5_hex($loc->{url}).'.'.$loc->{filetype};
	if (!save($file, $loc->{content})) {
	    error("cannot save local file");
	    $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
	    next;
	}

	# check if this is a subpage with further links to papers:
        if (is_subpage($loc)) {
            error("looks like a subpage with more links");
            $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
            next;
        }

	# check if this is an intermediate page leading to the actual paper:
        my $target = check_steppingstone($loc, $res);
        if ($target) {
            my ($is_dupe) = $dbh->selectrow_array(
                "SELECT 1 FROM locations WHERE url = ".$dbh->quote($target));
            if ($is_dupe) {
                error("steppingstone to already known location");
                $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
                next;
            }
            print "adding target link to queue: $target.\n" if $verbosity;
            $loc->{url} = $target;
            push @locations, $loc;
            # We could alternatively replace the URL in the database,
            # but then we'd constantly rediscover the steppingstone
            # link in the source page and treat it as new. Perhaps the
            # cleanest response would be to add the target location
            # separately to the database, with a link to the
            # redirecting location $loc. (Without such a link, the
            # location would appear to be an orphan.) To find the
            # source page of an article, one would then have to
            # retrieve a source page linking to a parent of (a parent
            # of ...) a location of the document. That's cumbersome,
            # so instead I simply overwrite the current location, but
            # keep its URL.
            next;
	}

	# get anchor text and default author from source page:
        # TODO: this assumes there is only one source page; I should
        # move this info into the XML file and let the spam filter
        # run on that. (See the 'TODO' comment in Spamfilter.pm.)
	$loc->{anchortext} = '';
        $loc->{default_author} = '';
        if ($loc_id) {
            ($loc->{anchortext}, $loc->{default_author}) = $dbh->selectrow_array(
                "SELECT links.anchortext, sources.default_author "
                ."FROM links INNER JOIN sources ON links.source_id = sources.source_id "
                ."WHERE links.location_id = $loc_id LIMIT 1");
        }

	# Except for html documents, we don't have the text content
        # yet, but we nevertheless do a preliminary spam test now,
        # so that we can stop processing if something is clear spam:
        if ($loc->{filetype} eq 'html') {
            $loc->{text} = strip_tags($loc->{content});
        }
	my $spamminess = 0;
        Spamfilter::cfg(\%cfg);
        Spamfilter::verbosity($verbosity);
	eval {
	    $spamminess = Spamfilter::classify($loc);
	};
	if ($@) {
	    error($@);
	    $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
	    next;
	}
	if ($spamminess > 0.5) {
	    if (defined $loc->{spamminess} && $loc->{spamminess} > 0.5) {
                print "was previously recognized as spam, still looks like that\n" if $verbosity;
		$db_verify->execute($loc_id);
		next;
	    }
	    if ($spamminess >= $cfg{'CERT_SPAM'}) {
                print "spam score $spamminess, not checking any further\n" if $verbosity;
                $db_saveloc->execute($loc->{filetype}, $loc->{filesize}, 
                                     $spamminess, undef, $loc_id)
                    or print DBI->errstr;
		next;
	    }
	}
        $loc->{spamminess} = $spamminess;

        # convert file to xml:
        Converter::cfg(\%cfg);
        Converter::verbosity($verbosity);
        eval {
            convert2xml($file);
        };
	if ($@) {
	    error("$@");
	    error("parser error") if errorcode() == 99;
	    $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
	    next;
	}

	# extract author, title, abstract:
        my $cand_title = $loc->{anchortext};
        $cand_title = '' if $cand_title !~ /\w\w/;
        $cand_title = '' if $cand_title =~ /$loc->{filetype}|version/i;
        my $cand_author = $loc->{default_author};
        # TODO: these values should have been put into the XML; see above.
	my $result;
	eval {
	    my $extractor = Extractor->new("$file.xml");
	    $extractor->verbosity($verbosity);
	    $extractor->prior('author', $cand_author, 0.7) if ($cand_author);
	    $extractor->prior('title', $cand_title, 0.5) if ($cand_title);
	    $result = $extractor->parse();
	};
	if ($@) {
	    error("$@");
	    error("parser error") if errorcode() == 99;
	    $db_err->execute(errorcode(), $loc_id) or print DBI->errstr;
	    next;
	}

	$loc->{authors} = force_utf8(join ', ', @{$result->{authors}});
	$loc->{title} = force_utf8($result->{title});
	$loc->{abstract} = force_utf8($result->{abstract});
	$loc->{confidence} = $result->{confidence};
	$loc->{length} = $result->{pages};
        $loc->{text} = $result->{text};
        # TODO: these should be added to the XML file by the extractor.

	# guess spamminess again, now that we have the text content:
	Spamfilter::verbosity($verbosity);
        eval {
	    $loc->{spamminess} = Spamfilter::classify($loc);
	};
        if ($loc->{spamminess} >= $cfg{'CERT_SPAM'}) {
            print "spam score $loc->{spamminess}, not storing document\n" if $verbosity;
            $db_saveloc->execute($loc->{filetype}, $loc->{filesize},
                                 $loc->{spamminess}, undef, $loc_id)
                or print DBI->errstr;
            next;
        }

	print "RESULT: $loc->{authors}: '$loc->{title}' "
            ."($loc->{filetype}, $loc->{filesize}b, $loc->{length}p, "
            ."spam: $loc->{spamminess}, conf: $loc->{confidence})\n"
            ."$loc->{abstract}\n\n" if $verbosity;

        if ($loc->{document_id}) {
            # location already links to a document; update the document:
            my ($old_confidence) = $dbh->selectrow_array(
                "SELECT meta_confidence FROM documents "
                ."WHERE document_id != $loc->{document_id} ");
            if ($loc->{confidence} <= $old_confidence) {
                print "results aren't more confident, nothing to update.\n" if $verbosity;
                next;
            }
            print "new confidence exceeds old one, updating document.\n" if $verbosity;
            $db_savedoc->execute($loc->{authors}, $loc->{title}, $loc->{abstract}, 
                                 $loc->{length}, $loc->{confidence}, $loc_id)
                or print DBI->errstr;
            $db_saveloc->execute($loc->{filetype}, $loc->{filesize},
                                 $loc->{spamminess}, $loc->{document_id}, $loc_id)
                or print DBI->errstr;
            next;
        }

        # check whether we already have this paper:
        my ($orig_id, $orig_url) = $dbh->selectrow_array(
            "SELECT documents.document_id, locations.url FROM documents "
            ."INNER JOIN locations ON documents.document_id = locations.document_id "
            ."WHERE status = 1 AND location_id != $loc_id "
            ."AND authors = ".$dbh->quote($loc->{authors})." "
            ."AND title = ".$dbh->quote($loc->{title})." "
            ."LIMIT 1");
        # better, with  the MySQL levenshtein UDF from joshdrew.com:
        #    ."AND levenshtein(authors,".$dbh->quote($loc->{authors}).") < 4 "
        #    ."AND levenshtein(title,".$dbh->quote($loc->{title}).") < 4 "
        #    ."AND levenshtein(abstract,".$dbh->quote($loc->{abstract}).") < 15 "
        if ($orig_id) {
            print "document already known at $orig_url.\n" if $verbosity;
            $db_saveloc->execute($loc->{filetype}, $loc->{filesize}, 
                                 $loc->{spamminess}, $orig_id, $loc_id)
                or print DBI->errstr;
            next;
        }

        # add document to database:
        next if $opts{n};
        $db_adddoc->execute($loc->{authors}, $loc->{title}, $loc->{abstract}, 
                            $loc->{length}, $loc->{confidence})
	    or print DBI->errstr;
        my $doc_id = $db_savedoc->{mysql_insertid};
        $db_saveloc->execute($loc->{filetype}, $loc->{filesize},
                             $loc->{spamminess}, $doc_id, $loc_id)
            or print DBI->errstr;

    }
}

sub is_subpage {
    my $loc = shift;
    return 0 unless $loc->{location_id}; # when called manually with URL parameter
    return 0 unless $loc->{filetype} eq 'html';
    # subpage must have high link density:
    my $numlinks = 0;
    $numlinks++ while ($loc->{content} =~ /<a href/gi);
    return 0 unless ($numlinks > 4 && ($numlinks/$loc->{filesize} > 0.002));
    # subpage must have at least three links of paper filetypes:
    $numlinks = 0;
    $numlinks++ while ($loc->{content} =~ /\.pdf|\.doc/gi);
    return 0 unless $numlinks > 2;
    # fetch potential parent pages:
    my $query = "SELECT sources.* FROM links "
        ."INNER JOIN sources ON links.source_id = sources.source_id "
        ."WHERE links.location_id = $loc->{id}";
    my @sources = @{$dbh->selectall_arrayref($query, { Slice => {} })};
    my @parents;
    foreach my $source (@sources) {
        # subpage must be located at same host and path as parent page:
        my $source_path = $source->{url};
        $source_path =~ s/[^\/]+\.[^\/]+$//; # strip filename
        next unless $loc->{url} =~ /^$source_path/;
        # parent page must not itself be subpage:
        next if $source->{parent_id};
        push @parents, $source;
    }
    return 0 unless @parents;
    # We can't properly handle subpages with multiple parents:
    if (scalar @parents > 1) {
        print "Oops, more than one candidate parent page!\n";
        return 0;
    }
    # Store page as new source:
    my $parent = pop @parents;
    my $db_addsub = $dbh->prepare(
        "INSERT IGNORE into sources (url, status, parent_id, default_author) "
        ."VALUES(?, 0, ?, ?)");
    $db_addsub->execute($loc->{url}, $parent->{source_id}, $parent->{default_author})
        or print DBI->errstr;
    return 1;
}

sub check_steppingstone {
    my $loc = shift;
    my $http_res = shift;
    return 0 unless $loc->{filetype} eq 'html';
    # catch intermediate pages that redirect with meta refresh
    # (e.g. http://www.princeton.edu/~graff/papers/ucsbhandout.html):
    my $target = '';
    if ($loc->{content} =~ /<meta.*http-equiv.*refresh.*content.*\n*url=([^\'\">]+)/i) {
        print "address redirects to $1.\n" if $verbosity;
        $target = URI->new($1);
        $target = $target->abs(URI->new($loc->{url}));
        $target =~ s/\s/%20/g; # fix links with whitespace
        return $target if length($target) < 256;
    }
    # other intermediate pages are short and have at least one link to a pdf file:
    return 0 if $loc->{filesize} > 5000;
    return 0 if $loc->{content} =~ /\.pdf/i;
    # prevent loops with redirects to login?req=foo.pdf
    return 0 if $loc->{url} !~ m/\.pdf$/i;
    print "might be a steppingstone page?\n" if $verbosity > 1;
    my $link_extractor = new HTML::LinkExtractor(undef, $http_res->base, 1);
    eval {
        $link_extractor->parse(\$http_res->{content});
    };
    my @as = grep { $$_{tag} eq 'a' && $$_{href} =~ /\.pdf$/ } @{$link_extractor->links};
    print Dumper @as if $verbosity > 1;
    return 0 if @as != 1; # want exactly one pdf link
    $target = ${$as[0]}{href};
    $target =~ s/\s/%20/g; # fix links with whitespace
    return $target if length($target) < 256;
}

sub force_utf8 {
    # when parsing PDFs, we sometimes get mess that is not valid UTF-8, which can
    # make the HTML/RSS invalid.
    my $str = shift;
    return $str if (Encode::is_utf8($str, 1));
    $str = decode 'utf8', $str;
    return $str if (Encode::is_utf8($str, 1));
    $str =~ s/[^\x{21}-\x{7E}]//g;
    return $str if (Encode::is_utf8($str, 1));
    return "";
}

