
opp-tools is a collection of tools to track academic papers uploaded
to (or linked from) certain web pages.

version 0.9 (2010-10-16)

---------------------------------------------------------------------
INSTALLATION AND SETUP
---------------------------------------------------------------------

Some additional software packages are required to handle documents of
various formats. (You may omit packages for formats you don't need.)

pdftohtml, the Poppler version   (all formats)
pdftk                            (all formats)
cuneiform 0.9                    (PDF/OCR, Postscript)
convert, from ImageMagick        (PDF/OCR, Postscript)
ps2pdf                           (Postscript)
openoffice.org                   (Word, RTF)
unoconv                          (Word, RTF)
wkhtmltopdf                      (HTML)

aspell-en libaspell-dev

On Ubuntu 10.04 and 10.10, the following installs everything except
cuneiform and wkhtmltopdf:

  sudo aptitude install poppler-utils pdftk imagemagick ghostscript \
  openoffice.org unoconv

The cuneiform version in the standard repositories too old: we need at
least 0.9. For Debian-based systems, you can get a recent version from
http://notesalexp.org/.

For wkhtmltopdf, I recommend a recent static binary from
http://code.google.com/p/wkhtmltopdf/, which you can install by
calling something like

  wget http://wkhtmltopdf.googlecode.com/files/wkhtmltopdf-0.9.9-static-i386.tar.bz2
  tar xvjf wkhtmltopdf-0.9.9-static-i386.tar.bz2
  sudo mv wkhtmltopdf-i386 /usr/bin/wkhtmltopdf

Finally, make sure you have the following CPAN Perl modules. You may
need to install gcc and openssl first:

  sudo aptitude install gcc openssl libssl-dev

  sudo perl -MCPAN -e 'install HTML::LinkExtractor'
  sudo perl -MCPAN -e 'install HTML::Encoding'
  sudo perl -MCPAN -e 'install HTML::Strip'
  sudo perl -MCPAN -e 'install HTML::TreeBuilder'
  sudo perl -MCPAN -e 'install XML::Writer'
  sudo perl -MCPAN -e 'install XML::XPath'
  sudo perl -MCPAN -e 'install XML::RSS'
  sudo perl -MCPAN -e 'install Text::Capitalize'
  sudo perl -MCPAN -e 'install String::Approx'
  sudo perl -MCPAN -e 'install Algorithm::NaiveBayes'
  sudo perl -MCPAN -e 'install AI::Categorizer'

Now set up a MySQL database to store the tracked pages and papers. To
create the tables, call

  mysql -u dbuser -p dbname < setup.sql 

Then rename config-example.pl to config.pl and adjust the values for
the database connection and the installation path.

At this point there aren't any pages in the database yet. To test the
setup, you might run

  ./update_sources add "http://consc.net/papers.html"
  ./process_pages -v1
  ./process_links -v1


---------------------------------------------------------------------
USAGE
---------------------------------------------------------------------

At the core of opp-tools lie two Perl modules, Converter and
Extractor. The Converter module converts documents from PDF, Word,
HTML and some other formats into XML. The Extractor module extracts
metadata from such XML documents. These two modules can be used on
their own, without any database or internet connection:

  #! /usr/bin/perl
  use Converter;
  use Extractor;
  my %cfg = do 'config.pl';
  Converter::cfg(\%cfg);
  Converter::convert2xml("test.pdf", "test.xml");
  my $extractor = Extractor->new("test.xml");
  my $result = $extractor->parse();
  print "authors: ", join(', ', @{$result->{authors}}), "\n";
  print "title: ", $result->{title}, "\n";

The rest of the package makes use of the Converter and Extractor
modules to keep track of documents that are linked to from certain web
pages. The tracked pages are stored in the "sources" table of the
database. Instead of manually editing the MySQL table, you can use
update_sources, as in:

  ./update_sources add "http://consc.net/papers.html"

See

  ./update_sources -h

for more usage information.

The two main scripts for retrieving document information from the
source pages are process_pages and process_links. The first checks for
links on the source pages and stores them in the "locations" table in
the database. The second fetches the linked documents, extracts
metadata and stores the result in the "documents" table. The two
scripts are meant to be called regularly, e.g. every 5 minutes, by a
cron job, so that the database is always up-to-date with the source
pages. 

Often a source page will not only contain links to papers but also to
irrelevant stuff like a department home page or a CV. To filter these
out, process_links assigns a "spamminess" score between 0 and 1 to
each processed URL; the score is stored in the "locations" table. It
is calculated by a combination of ad hoc heuristics and a Bayesian
classifier.

The classifier comes pre-trained with a small "ham" and "spam" corpus,
which is included in the spamcorpus directory. To adjust the filter,
you can add or remove documents in the ham and spam directories and
then run

  ./train_filter

in the spamcorpus directory. The documents in the ham and spam
directory should be in plain text format. To add e.g. a PDF or HTML
document, you can run

  ./add_document -s http://example.com/department/index.html

This adds the text content of the given URL to the spam
corpus. Without the -s flag, the content is added to the ham corpus.

The Extractor module that extracts metadata from a document also
estimates its own accuracy for a given document. This is stored as
"meta_confidence" in the "documents" table. A confidence value of 0.5
suggests that it is roughly as likely that the extracted values are
correct than not.

It is up to you to do anything further with the contents of the
"documents" table. Included is a script called "rss" that generates an
RSS feed of the most recently found documents.


---------------------------------------------------------------------
SMALL PRINT 
---------------------------------------------------------------------

The development of this software will be partly supported by the
University of London and the UK Joint Information Systems Committee as
part of the PhilPapers 2.0 project (Information Environment
programme).

Copyright (c) 2003-2010 Wolfgang Schwarz, wo@umsu.de

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or (at
your option) any later version. See
http://www.gnu.org/licenses/gpl.html.

This program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
General Public License for more details.

