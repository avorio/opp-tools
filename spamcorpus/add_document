#! /usr/bin/perl -w
use strict;
use warnings;
binmode STDOUT, ":utf8";
use DBI;
use Digest::MD5;
use Getopt::Std;
use lib '../';
use util::Io;
use Converter;
my %cfg = do 'config.pl';

$cfg{'TEMPDIR'}       = $cfg{'PATH'}.'temp/';
$cfg{'RPDF'}          = $cfg{'PATH'}.'rpdf/rpdf';
$cfg{'RTF2PDF'}       = $cfg{'PATH'}.'util/rtf2pdf.sh';

my %opts;
getopts("s", \%opts);
if (!@ARGV) {
    print <<EOF;

Add a document to the ham or spam corpus. This merely extracts the
plain text content from a document and saves it in the 'ham' or 'spam'
folder. You may also manually add text documents into these
folders. Call train_filter to train the filter on the present corpus.

Usage: $0 [-s] <url or id>

-s        : add to spam corpus (default is ham)

EOF
    exit;
}

my $cat = $opts{'s'} ? 'spam' : 'ham';
my $url = shift @ARGV;
if ($url =~ /^\d+$/) {
    my $dbh = DBI->connect('DBI:mysql:'.$cfg{'MYSQL_DB'}, $cfg{'MYSQL_USER'},
                           $cfg{'MYSQL_PASS'}, { RaiseError => 1 }) 
        or die "Couldn't connect to database: " . DBI->errstr;
    ($url) = $dbh->selectrow_array(
        "SELECT url FROM locations WHERE document_id = '$url' LIMIT 1");
    die "ID not in database" unless ($url);
}

# Ideally, I would now run most of process_links, construct the XML
# representation of all relevant features, and run the spam classifier
# with all the separate features. For now, I just convert the raw
# content to text and treat it as a single blob.

print "fetching $url.\n";
my $res = fetch_url($url);
if (!$res || !$res->is_success || !$res->content) {
    print "Download failed.\n";
    exit;
}
my $filetype = $res->{filetype};
my $fname = Digest::MD5::md5_hex($url);
my $file = $cfg{'TEMPDIR'}.$fname.'.'.$filetype;
if (!save("$file", $res->content)) {
    error("cannot save file $file");
}
Converter::cfg(\%cfg);
Converter::verbosity(2);
my $text = convert2text("$file");
save("$cat/$fname", $text);
