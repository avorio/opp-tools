#! /usr/bin/perl -w
use strict;
use warnings;
use FindBin qw($Bin);
use Cwd 'abs_path';
use File::Basename 'dirname';
use DBI;
use Digest::MD5;
use Getopt::Std;
use lib '../';
use util::Io;
use Converter;

binmode STDOUT, ":utf8";

my $path = dirname(abs_path(__FILE__));
my %cfg = do "$path/../config.pl";
my $TEMPDIR = "$path/../temp";

my %opts;
getopts("s", \%opts);
if (!@ARGV) {
    print <<EOF;

Add a document to the ham or spam corpus. This merely extracts the
plain text content from a document and saves it in the 'ham' or 'spam'
folder. You may also manually add text documents into these
folders. Call train_filter to train the filter on the present corpus.

Usage: $0 [-s] <url or id>

-s        : add to spam corpus (default is ham)

EOF
    exit;
}

my $cat = $opts{'s'} ? 'spam' : 'ham';
my $url = shift @ARGV;
if ($url =~ /^\d+$/) {
    my $dbh = DBI->connect(
        'DBI:mysql:'.$cfg{'MYSQL_DB'}, $cfg{'MYSQL_USER'},
        $cfg{'MYSQL_PASS'}, { RaiseError => 1 }) 
        or die "Couldn't connect to database: " . DBI->errstr;
    ($url) = $dbh->selectrow_array(
        "SELECT url FROM locations WHERE document_id = '$url' LIMIT 1");
    die "ID not in database" unless ($url);
}

# Ideally, I would now run most of process_links, construct the XML
# representation of all relevant features, and run the spam classifier
# with all the separate features. For now, I just convert the raw
# content to text and treat it as a single blob.

print "fetching $url.\n";
my $res = fetch_url($url);
if (!$res || !$res->is_success || !$res->content) {
    print "Download failed.\n";
    exit;
}

my $filetype = $res->{filetype};
my $fname = Digest::MD5::md5_hex($url);
my $file = $TEMPDIR.'/'.$fname.'.'.$filetype;
if (!save("$file", $res->content)) {
    error("cannot save file $file");
}
Converter::verbosity(2);

my $text = convert2text("$file");
save("$path/$cat/$fname", $text);
