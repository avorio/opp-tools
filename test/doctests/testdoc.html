<P>
<TITLE>The Two-Envelope Paradox: A Complete Analysis?</TITLE>
<BODY TEXT="#000000" BGCOLOR="#FFFFFF" LINK="#0000EE" VLINK="#551A8B" ALINK="#FF0000">

<H2>The Two-Envelope Paradox: A Complete Analysis?</H2>
<H3> <A HREF = "http://www.u.arizona.edu/~chalmers/">
David J. Chalmers </A></H3>

<B>Department of Philosophy<BR>
University of Arizona<BR>
Tucson, AZ 85721<P>

<A HREF="mailto:chalmers@arizona.edu">chalmers@arizona.edu</A></B><P>

[[<font size = -1>Doug Hofstadter introduced me to the two-envelope paradox
in 1988.  This paper corresponds to more or less the position I came up with then.  I wrote this up in 1994 after a couple of papers on the subject appeared in <i>Analysis</i>.  I never published it, partly because it came to seem to me that this treatment resolves only part of the paradox: it resolves the "numerical" paradox but not the "decision-theoretic" paradox.  For a more recent treatment of the decision-theoretic paradox, see <a href="stpete.html">The St. Petersburg Two-Envelope Paradox</a>.</font>]]</p>

<P>
A wealthy eccentric places two envelopes in front of you.  She tells you
that both envelopes contain money, and that one contains twice as much as
the other, but she does not tell you which is which.  You are allowed
to choose one envelope, and to keep all the money you find inside.
<P>
This may seem innocuous, but it generates an apparent paradox.  Say that
you choose envelope 1, and it contains $100.  In evaluating your decision,
you reason that there is a 50% chance that envelope 2 contains $200, and
a 50% chance that it contains $50.  In retrospect, you reason, you should
have taken envelope 2, as its expected value is $125.  If your sponsor
offered you the chance to change your decision now, it seems that you
should do so.  Now, this reasoning is independent of the actual amount in
envelope 1, and in fact can be carried out in advance of opening the
envelope; it follows that whatever envelope 1 contains, it would be better
to choose envelope 2.  But the situation with respect to the two envelopes 
is symmetrical, so the same reasoning tells you that whatever envelope 2
contains, you would do better to choose envelope 1.  This seems
contradictory.  What has gone wrong?
<P>
The paradox can be expressed numerically.  Let <I>A</I> and <I>B</I> be the amounts
in envelope 1 and 2 respectively; their expected values are <I>E(A)</I> and
<I>E(B)</I>.  For all <I>n</I>, it seems that <I>p(B>A|A=n) = 0.5</I>, so that <I>E(B|A=n) =
1.25n</I>.  It follows that <I>E(B)=1.25E(A)</I>, and therefore that <I>E(B) > E(A)</I>
if either expected value is greater than zero.  The same reasoning shows
that <I>E(A) > E(B)</I>, but the conjunction is impossible, and in any case
<I>E(A) = E(B)</I> by symmetry.  Again, what has gone wrong?
<P>
This problem has been discussed in the pages of <I>Analysis</I> by Jackson,
Menzies and Oppy [2], and by Castell and Batens [1], but for reasons that
will become clear I think that their analyses are incomplete and mistaken
respectively, although both contain insights that are important to the
resolution of the problem.  I will therefore present my own analysis of the
"paradox" below.
<P>
Some distractions inessential to the problem arise from the facts that in
the real world, money comes in discrete amounts (dollars and cents, pounds
and pence) and that there are known limits on the world's money supply.  We
can remove these distractions by stipulating that for the purposes of the
problem, the amounts in the envelopes can be any positive real number.
<P>
There are a number of steps in the resolution of the paradox.  The first
step is to note (as do the authors mentioned above) that the amounts in the
envelopes do not fall out of the sky, but must be drawn from some
probability distribution.  Let the relevant probability density function be
<I>g</I>, where the probability that the smaller amount falls between <I>a</I> and
<I>b</I> is <I>integral[a,b] g(x) dx</I>.  We can think of this distribution as either
representing the chooser's prior expectations, or as the distribution from
which the actual values are drawn.  I will generally write as if it is the
second, but nothing much rests on this.  To fix ideas, we can imagine that
our sponsor chooses a random variable <I>Z</I> with probability density <I>g</I>, and
then flips a coin.  If the coin comes up heads, she sets <I>A=Z</I> and <I>B=2Z</I>;
if it comes up tails, she sets <I>A=2Z</I> and <I>B=Z</I>.
<P>
Recognizing the existence of a distribution immediately shows us that the
reasoning that leads to the paradox is not always valid, as Jackson <I>et al</I>
note.  For example, if the distribution is a uniform distribution over
values between 0 and 1000, with amounts over 1000 being impossible, then if
<I>A > 500</I>, it is always a bad idea to switch.  It is therefore not true
that for all distributions and all values of <I>n</I>, <I>p(B>A|A=n) = 0.5</I>.  In
general, <I>E(B|A=n)</I> will not depend only on <I>n</I>; it will also depend on the
underlying distribution.
<P>
In their analysis, Jackson <I>et al</I> are satisfied with this observation,
combined with the observation that limitations on the worlds' money supply
ensure that in practice the relevant distributions will always be bounded
above and below.  The paradox does not arise for bounded distributions, as
we saw above.  When <I>A</I> is a medium value, there may be equal chances that <I>B</I>
is larger or smaller, but when <I>A</I> is large <I>B</I> is likely to be smaller, and
when <I>A</I> is small <I>B</I> is likely to be larger, so the paradox does not get off
the ground.
<P>
This practical observation is an insufficient response to the mathematical
paradox, however, as Castell and Batens note.  Unbounded distributions can
exist in principle if not in practice, and in-principle existence is all
that is needed for the paradox to have its bite.  For example, it might
seem that if the distribution were a uniform distribution over the real
numbers, then <I>p(B>A|A=n) = 0.5</I> for all <I>n</I>.  This would seem to have
paradoxical consequences for mathematics, if not for the world's money
supply.
<P>
This leads to the second step in the resolution of the paradox, which is
that taken by Castell and Batens.  (We will see that this step is
ultimately inessential to the paradox's resolution, but it is an important
intermediate point of enlightenment.)  There is in fact no such thing as a
uniform probability distribution over the real numbers.  To see this, let
<I>g</I> be a uniform function over the real numbers.  Then <I>integral[k,k+1]
g(x)dx</I> is equal to some constant <I>c</I> for all <I>k</I>.  If <I>c=0</I>, then the area
under the entire curve will be zero, and if <I>c>0</I>, then the area under the
entire curve will be infinite, both of which contradict the requirement
that the integral of a probability distribution be 1.  At one point Jackson
<I>et al</I> raise the possibility of infinitesimal probabilities, but if this
is interpreted as allowing <I>c</I> to be infinitesimal, the suggestion does not
work any better.  To see this, note that if the distribution is uniform:<p>
<i>integral[0, infinity] g(x) dx<br> = integral[0,1] g(x)dx + integral[1,2] g(x)dx + integral[2,3] g(x)dx + ...<br> = integral[0,1] g(x)dx + integral[2,3] g(x)dx + integral[4,5] g(x)dx <br> = (integral[0,infinity] g(x)dx)/2</i><p>

so that the overall integral must be zero or infinite.  A uniform
distribution over the real numbers can only be an "improper" distribution,
whose overall integral is not 1.
<P>
The impossibility of a uniform probability distribution over the real
numbers is reflected in the fact that every proper distribution must
eventually "taper off": for all <I>epsilon > 0</I>, there must exist <I>k</I> such
that <I>integral[k, infinity] g(x)dx < epsilon</I>.  It is very tempting to
suppose that this "tapering off" supplies the resolution to the paradox, as
it seems to imply that if <I>A</I> is near the high end of the (proper)
distribution, it will be more likely that <I>B</I> is smaller; perhaps
sufficiently more likely to offset the paradoxical reasoning?  This is
the conclusion that Castell and Batens draw.  They offer a "proof" that the
distribution must be improper for the paradoxical reasoning to be possible.
<P>
Unfortunately Castell and Batens' proof is mistaken, and in fact there
exist proper distributions for which the paradoxical reasoning is possible.
The error lies in their assumption, early in the paper, that <i>p(B>A|A=n) =
g(n)/(g(n) + g(n/2)).</I>  This seems intuitively reasonable, but in
fact <I>p(B>A|A=n) = 2g(n)/(2g(n) + g(n/2))</i>, which is significantly
larger in general.
<P>
To see this, note that if <I>A</I> is in the range <I>n +/- dx</I>, then <I>B</I> is
either in the range <I>2n +/- 2dx</I> or in the range <I>n/2 +/- dx/2</I>.  The
probability of the first, relative to the initial distribution, is
<I>g(n)dx</I>; the probability of the second is <I>g(n/2)dx/2</I>.  The probabilities
that <I>B</I> is greater or less than <I>A</I> therefore stand in the ratio
<I>2g(n):g(n/2)</I>, not <I>g(n):g(n/2)</I>, as Castell and Batens suppose.
<P>
For example, given a uniform distribution between 0 and 1000, if <I>A</I> is
around 100, it is in fact twice as likely that <I>B</I> is around 200 than that
<I>B</I> is around 50.  To dispel any lingering counterintuitiveness, note that
something like this <I>has</I> to be the case to make up for the fact that when
<I>A > 500</I>, <I>B</I> is always less than <I>A</I>.  To find a distribution where the
chances of a gain and a loss are truly equal for many <I>n</I>, we should turn
not to a uniform distribution but to a decreasing distribution, where
<I>g(n/2) = 2g(n)</I> for many <I>n</I>.  An example is the distribution <I>g(x) =
1/x</I>, where we cut off the distribution between arbitrary bounds <I>L</I> and
<I>U</I>, and normalize so that it has an integral of 1.  This distribution will
have the property that for all <I>n</I> such that <I>2L < n < U/2</I>, <I>p(B>A|A=n) =
0.5</I>.  To illustrate this intuitively, note that for such a decreasing
distribution, the prior probability that the smaller value is between 4 and
8 is the same as the probability that it is between 8 and 16, and so on, if
<I>L</I> and <I>U</I> are appropriate.  Given the information that <I>8 < A < 16</I>, it
is equally likely that <I>B</I> is in the range above or below.
<P>
This flaw in Castell and Batens' reasoning nullifies their proof that
a distribution must be improper for the paradoxical reasoning to
arise, but it does not yet show that the conclusion is false.  It
remains open whether there is a proper distribution for which the
paradoxical reasoning is possible.  The bounded distribution above
will not work, as its bound will block the paradoxical reasoning in
the usual fashion; and the <I>unbounded</I> distribution <I>g(x) = 1/x</I> is
improper, having an infinite integral.  But this can easily be fixed,
by allowing the distribution to taper off slightly faster.  In
particular, the distribution <I>g(x) = x^(-1.5)</I>, cut off below a lower
bound <I>L</I> and normalized, allows the paradox to arise.  The
distribution has a finite integral, and even though for most <I>n</I>,
<I>p(B>A|A=n) < 0.5</I>, it is still the case that for all relevant <I>n</I>,
<I>E(B|A=n) > n</I>.  To see this, note that if <I>n < 2L</I>, then <I>E(B|A=n) =
2n</I>; and if <I>n &gt= 2L</I>, then<p>

<i>p(B>A|A=n) : p(B < A|A=n)<br>
                        =  2g(n):g(n/2) <BR>
                        = 2n^(-1.5):(n/2)^(-1.5) <BR>
                        = 1:sqrt(2).</i><p>

The expected value <I>E(B|A=n)</I> is <I>(2n+sqrt(2)n/2)/(1+sqrt(2))</I>, which is
about <I>1.12n</I>.  The paradox therefore still arises.
<P>
The distribution here may be unintuitive, but it is easy to illustrate a
similar distribution intuitively.  Take a distribution in which the
probability of a value between 1 and 2 is <I>c</I>, the probability of a value
between 2 and 4 is just slightly less, say <I>0.9c</I>, the probability of a
value between 4 and 8 is <I>0.81c</I>, and so on.  This distribution has a
finite integral, as the integral is the sum of a decreasing geometric
series; and it is sufficiently close to the case in which the probability
of a value between <I>2^k</I> and <I>2^(k+1)</I> is constant that the paradoxical
reasoning still arises.  Even though <I>p(B &lt A|A=n)</I> is now slightly less than
0.5, due to the incorporated factor of 0.9, it has decreased by a
sufficiently small amount that <I>E(B|A=n)</I> remains greater than <I>n</I>.  The
case <I>g(x) = x^(-1.5)</I> is just like this, except that the factor of 0.9 is
replaced by a factor of <I>1/sqrt(2)</I>, which is around 0.7.
<P>
The paradox has therefore not yet been vanquished; there are perfectly
proper distributions for which the paradoxical reasoning still applies.
This leads us to the third and final step in the resolution of the paradox.
Note that although the distributions above have finite integrals, as a
probability distribution should, they have infinite <I>expected value</I>.  The
expected value of a distribution is <I>integral[0,infinity] xg(x)dx</I>.  When <I>g(x) =
x^(-1.5)</I> (cut off below <I>L</I>), the expected value is <I>integral[L,infinity]
x^(-0.5) dx</I>, which is infinite.  But if the expected value of the
distribution is infinite, there is no paradox!  There is no contradiction
between the facts that <I>E(B) = 1.12 E(A)</I> and <I>E(A) = 1.12 E(B)</I> if both
<I>E(A)</I> and <I>E(B)</I> are infinite.  Rather, we have just another example of a
familiar phenomenon, the strange behavior of infinity.[*]
<P>
*[[[Castell and Batens note some similar consequences of infinite expected
values in another context, in which the distribution is over a countable
set.  They say that infinite expected values are "absurd", but I do not see
any mathematical absurdity.]]]
<P>
To fully resolve the paradox, we need only demonstrate that for
distributions with finite expected value, the paradoxical situation does
not arise.  To do this, we need to precisely state the conditions
expressing the paradoxical situation.  In its strongest form, the
paradoxical situation arises when <I>E(B|A=n) > n</I> for all <I>n</I>.  However, it
arises more generally whenever reasoning from <I>B</I>'s dependence on <I>A</I> leads
us to the conclusion that there is expected gain <I>on average</I> (rather than
all the time) by switching <I>A</I> for <I>B</I>.  This will hold whenever <I>E(K-A) >
0</I>, where <I>K</I> is the random variable derived from <I>A</I> by the transformation
<I>x -> E(B|A=x)</I>.  We therefore need to show that when <I>E(A)</I> is
finite, <I>E(K-A) = 0</I>.
<P>
Let <I>h</I> be the density function of <I>A</I>.  Then <I>h(x) = (g(x) + g(x/2)/2)/2
= (2g(x)+g(x/2))/4</I>.  (Note that <I>h != g</I>, as <I>g</I> is the density function
of the <I>smaller</I> value.)  Then 
<p><i>
E(K-A) = integral[0,infinity] h(x) (E(B|A=x) - x) dx <BR>
       = integral[0,infinity] (2g(x) + g(x/2))/4 . ((2x.2g(x) + x/2.g(x/2))/(2g(x)+g(x/2)) - x) dx <BR>
       = integral[0,infinity] (2xg(x) - x/2 . g(x/2))/4 dx <BR>
       = (integral[0,infinity] 2xg(x)dx - integral[0,infinity] 2yg(y)dy)/4 <BR>
       = 0.
</i><p>
Note that the fourth and fifth steps above are valid only if <I>integral[0,infinity]
xg(x)dx</I> is finite, which holds iff <I>E(A)</I> is finite.  (If <I>integral[0,infinity]
xg(x)dx</I> is infinite, it is possible that <I>integral[0,infinity]
2xg(x)-x/2.g(x/2)dx != 0</I>, even though <I>integral[0,infinity] 2xg(x)dx =
integral[0,infinity] x/2.g(x/2) dx</I>.)
<P>
It follows that when <I>E(A)</I> is finite, consideration of the dependence
of <I>B</I> on <I>A</I> will not lead one to the conclusion that one should switch
<I>A</I> for <I>B</I>.  A colollary of the result is that when <I>E(A)</I> is finite, it
is impossible that <I>E(B|A=n) > n</I> for all <I>n</I>, so that the strong form of
the paradox certainly cannot arise.
<P>
If <I>E(A)</I> is infinite, this result does not hold.  In such a case, it is
possible that <I>E(A) = E(K)</I> (both are infinite) but that <I>E(K-A) > 0</I>.
Here, the "paradoxical" reasoning will indeed arise.  But now the result is
no longer paradoxical; it is merely counterintuitive.  It is a consequence
of the fact that given infinite expectations, <I>any</I> given finite value will
be disappointing.  The situation here is somewhat reminiscent of the
classical St. Petersburg paradox: both "paradoxes" exploit random variables
whose values are always finite, but whose expected values are infinite.
The combination of finite values with infinite expected values leads to
counterintuitive consequences, but we cannot expect intuitive results where
infinity is concerned.[*]
<P>

<H3>References</H3>
<P>
[1] P. Castell and D. Batens, `The Two-Envelope Paradox: The Infinite
Case'.  <I>Analysis</I> 54:46-49.
<P>
[2] F. Jackson, P. Menzies, and G. Oppy, `The Two Envelope "Paradox"',
<I>Analysis</I> 54:43-45.
<!-- Start of StatCounter Code -->
<script type="text/javascript" language="javascript">
var sc_project=513930; var sc_partition=3; var sc_invisible=1; </script>

<script type="text/javascript" language="javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><a href="http://www.statcounter.com/" target="_blank"><img  src="http://c4.statcounter.com/counter.php?sc_project=513930&amp;amp;java=0&amp;amp;invisible=1" alt="website statistics" border="0"></a> </noscript>
<!-- End of StatCounter Code -->
</BODY>
