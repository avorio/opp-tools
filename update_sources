#!/usr/bin/perl
use strict;
use warnings;
use CGI;
use DBI;
use Getopt::Long;
use Encode;
binmode STDOUT, ":utf8";
my %cfg = do 'config.pl';

my $usage = <<EOF;

Update the list of tracked pages. Can be called from the command-line
or as a CGI script. 

Command-line usage: 

$0 [options] <action> <id>

<action> : one of "add", "delete", "modify"
<id>     : ID or URL of source page

Options (only relevant for "add" and "modify"):
--author : set default_author
--crawl  : set crawl_depth
--url    : set new URL

Example:
$0 add "http://example.com" --author "Paul Celan"

For use via CGI, pass the analogous HTTP parameters "action", "id" and
optionally "author", "crawl", "url". The script then outputs a JSON
object like

   { status: 1, msg: "http://example.com deleted" }

where "status" is 0 (failure) or 1 (success), and "msg" is an 
explanatory string.

EOF


my $cgi = new CGI;
$cgi->charset("utf-8");
my $is_cgi = $cgi->param('action');

sub ret {
    my ($status, $msg) = @_;
    if ($is_cgi) {
        print "{ \"status\":\"$status\", \"msg\":\"$msg\" }\n";
        exit;
    }
    print "".($status ? "OK" : "Error").": $msg\n";
    exit $status;
}

my $action = '';
my $id = '';
my $author;
my $url;
my $crawl;

if ($is_cgi) {
    print $cgi->header('text/plain');
    unless ($ENV{REMOTE_ADDR} =~ /$cfg{ALLOWED_IPS}/) {
        ret(0, "unauthorised access from $ENV{REMOTE_ADDR}") 
    }
    $action = $cgi->param('action');
    $id = $cgi->param('id');
    $author = $cgi->param('author') if $cgi->param('author');
    $crawl = $cgi->param('crawl') if $cgi->param('crawl');
    $url = $cgi->param('url') if $cgi->param('url');
    ret(0, "id/url parameter missing") unless $id;
}
else {
    if (scalar @ARGV < 2) {
        print $usage; 
        exit;
    }
    $action = $ARGV[0];
    $id = $ARGV[1];
    GetOptions('author=s' => \$author,
               'crawl=i' => \$crawl,
               'url=s' => \$url);
}

my $dbh = DBI->connect('DBI:mysql:'.$cfg{'MYSQL_DB'}, $cfg{'MYSQL_USER'},
    $cfg{'MYSQL_PASS'}, { RaiseError => 1 }) 
    or ret(0, "Could not connect to database");

$dbh->{'mysql_enable_utf8'} = 1;
$dbh->do("SET NAMES 'utf8'");

my $dispatch = {};

my $db_add = $dbh->prepare(<<SQL);
   INSERT IGNORE INTO sources
   (url, status, default_author)
   VALUES(?, 0, ?)
SQL

my $db_set_author = $dbh->prepare(<<SQL);
   UPDATE sources SET default_author = ? WHERE source_id = ?
SQL

my $db_set_crawl = $dbh->prepare(<<SQL);
   UPDATE sources SET crawl_depth = ? WHERE source_id = ?
SQL

my $db_set_url = $dbh->prepare(<<SQL);
   UPDATE sources SET url = ? WHERE source_id = ?
SQL

$dispatch->{add} = sub {
    my $res = $db_add->execute($id, $author)
        or ret(0, DBI->errstr || "MySQL execution failed");
    ret(0, "$id already in database") if ($res eq '0E0');
    my $source_id = $db_add->{mysql_insertid};
    ret(0, "insert failed") unless $source_id;
    if (defined($crawl)) {
        # If crawl is included in the add command, then undefined
        # crawl values are treated as 0. So we treat it separately.
        $db_set_crawl->execute($crawl, $id)
            or ret(0, DBI->errstr || "set failed");
    } 
    ret(1, "inserted as source id $source_id");
};

my $db_delete_docs = $dbh->prepare(<<SQL);
   DELETE documents.* FROM documents
   INNER JOIN locations ON documents.document_id = locations.document_id
   INNER JOIN links ON locations.location_id = links.location_id
   WHERE links.source_id = ?
   AND NOT documents.document_id IN
      (SELECT document_id FROM locations 
       GROUP BY document_id HAVING COUNT(document_id) > 1)
   AND NOT locations.location_id IN
      (SELECT location_id FROM links
       GROUP BY location_id HAVING COUNT(location_id) > 1)
SQL

my $db_delete_locs = $dbh->prepare(<<SQL);
   DELETE locations.*
   FROM locations
   INNER JOIN links ON locations.location_id = links.location_id
   WHERE links.source_id = ?
   AND NOT locations.location_id IN
      (SELECT location_id FROM links
       GROUP BY location_id HAVING COUNT(location_id) > 1)
SQL

my $db_delete_links = $dbh->prepare(<<SQL);
   DELETE FROM links WHERE source_id = ?
SQL

my $db_delete_page = $dbh->prepare(<<SQL);
   DELETE FROM sources WHERE source_id = ?
SQL

sub get_children {
    my $id = shift;
    return $dbh->selectall_arrayref(
        "SELECT source_id FROM sources WHERE parent_id = ".$dbh->quote($id),
        { Slice => {} });
}

$dispatch->{delete} = sub {
    my $del_id = shift || $id;
    my $recursion = shift || 0;
    my $keep_documents = shift || 0; # used in {modify}
    if ($del_id !~ /^\d+$/) {
        ($del_id) = $dbh->selectrow_array(
            "SELECT source_id FROM sources WHERE url = ".$dbh->quote($del_id));
        ret(0, 'Address not in database') unless $del_id;
    }
    # first delete all documents linked only from this page:
    unless ($keep_documents) {
        $db_delete_docs->execute($del_id)
            or ret(0, DBI->errstr || "delete failed");
    }
    # then delete all locations and links unique to this page:
    $db_delete_locs->execute($del_id)
        or ret(0, DBI->errstr || "delete failed");
    $db_delete_links->execute($del_id)
        or ret(0, DBI->errstr || "delete failed");
    # then recursively delete all child pages:
    my $children = get_children($del_id);
    foreach my $child (@{$children}) {
        $dispatch->{delete}($child->{source_id}, $recursion+1);
    }
    # then delete the page itself:
    $db_delete_page->execute($del_id)
        or ret(0, DBI->errstr || "delete failed");
    ret(1, "$del_id deleted") unless $recursion;
};

$dispatch->{modify} = sub {
    if ($id !~ /^\d+$/) {
        ($id) = $dbh->selectrow_array(
            "SELECT source_id FROM sources WHERE url = ".$dbh->quote($id));
        ret(0, 'Address not in database') unless $id;
    }
    my ($cur_crawl, $cur_author, $cur_url) = $dbh->selectrow_array(
        "SELECT crawl_depth, default_author, url FROM sources "
        ."WHERE source_id = $id")
        or ret(0, DBI->errstr || "ID not in database");
    $cur_author = '' unless $cur_author;
    if (($url && $url ne $cur_url) ||
        (defined($crawl) && $crawl != $cur_crawl)) {
        # If the URL changed, child pages probably changed their
        # location as well (or even disappeared); so we delete the
        # current children and let any new ones be rediscovered. We
        # also delete the current children if the crawl depth
        # changed. In the rare event that the depth is increased from
        # 1 or decreased from a value > 1, this means that some
        # children will be rediscovered. In either case, we don't
        # delete any documents, so no old documents will show up as
        # new.
        my $children = get_children($id);
        foreach my $child (@{$children}) {
            $dispatch->{delete}($child->{source_id}, 1, 1);
        }
    }
    # update the source itself:
    if (defined($author) && $author ne $cur_author) {
        # recursively change default_author of subpages:
        print "changing $id ";
        change_author($id, $author);
    }
    if (defined($crawl) && $crawl ne $cur_crawl) {
        $db_set_crawl->execute($crawl, $id)
            or ret(0, DBI->errstr || "set failed");
    }
    if (defined($url) && $url ne $cur_url) {
        $db_set_url->execute($url, $id)
            or ret(0, DBI->errstr || "set failed");
    }
    ret(1, "$id modified");
};

sub change_author {
    my $mod_id = shift;
    my $author = shift;
    # passing an empty author string sets default_author to NULL:
    $author = undef unless $author;
    my $children = get_children($mod_id);
    foreach my $child (@{$children}) {
        change_author($child->{source_id}, $author);
    }
    $db_set_author->execute($author, $mod_id)
        or ret(0, DBI->errstr || "set failed");
}

ret(0, "unknown action: $action") unless defined($dispatch->{$action});
&{$dispatch->{$action}};
